{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8b0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee5e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (246008, 239)\n",
      "Y_train (246008, 1)\n",
      "Y_test (61503, 1)\n",
      "ids_test (246008, 1)\n",
      "feature names (239, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('../2_INPUT_DATA/3_SPLIT/X_train.csv')\n",
    "Y_train = pd.read_csv('../2_INPUT_DATA/3_SPLIT/Y_train.csv')\n",
    "X_test = pd.read_csv('../2_INPUT_DATA/3_SPLIT/X_test.csv')\n",
    "Y_test = pd.read_csv('../2_INPUT_DATA/3_SPLIT/Y_test.csv')\n",
    "ids_test = pd.read_csv('../2_INPUT_DATA/3_SPLIT/ids_test.csv')\n",
    "feature_names = pd.read_csv('../2_INPUT_DATA/2_FEATURE_PROCESSED/feature_names.csv')\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"Y_train\", Y_train.shape)\n",
    "print(\"Y_test\", Y_test.shape)\n",
    "print(\"ids_test\",ids_test.shape)\n",
    "print(\"feature names\", feature_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bede7c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 99000.0,\n",
       " 490495.5,\n",
       " 27517.5,\n",
       " 454500.0,\n",
       " 0.035792,\n",
       " 16941,\n",
       " -1588.0,\n",
       " -4970.0,\n",
       " -477,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2.0,\n",
       " 2,\n",
       " 2,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.3542247319929012,\n",
       " 0.6212263380626669,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -2536.0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = X_train.copy()\n",
    "test[\"ID\"] = ids_test\n",
    "test.set_index(\"ID\", inplace=True)\n",
    "ids_test.iloc[5]\n",
    "test.loc[100008].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee29821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae50308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 226067, 1: 19941})\n",
      "Counter({0: 45212, 1: 22606})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def SMOTE_transformation(X, y):\n",
    "    # summarize class distribution\n",
    "    counter = Counter(y)\n",
    "    print(counter)\n",
    "    # define pipeline\n",
    "    over = SMOTE(sampling_strategy=0.1)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    # transform the dataset\n",
    "    X, y = pipeline.fit_resample(X, y)\n",
    "    # summarize the new class distribution\n",
    "    counter = Counter(y)\n",
    "    print(counter)\n",
    "    # scatter plot of examples by class label\n",
    "    #for label, _ in counter.items():\n",
    "    #    print(label)\n",
    "    #    row_ix = where(y == label)[0]\n",
    "    #    print(row_ix)\n",
    "    #    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "        \n",
    "    #pyplot.legend()\n",
    "    #pyplot.show()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "x_train_smote, y_train_smote = SMOTE_transformation(X_train, Y_train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1cdbe1",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "**Fbeta Score**\n",
    "\n",
    "![alt text for screen readers](IMAGES/fbetascore.png \"Fbeta Score\")\n",
    "\n",
    "A smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score.\n",
    "\n",
    "#### FN et FP  \n",
    "Vous pourrez supposer, par exemple, que le coût d’un FN est dix fois supérieur au coût d’un FP.  \n",
    "Vous créerez un score “métier” (minimisation du coût d’erreur de prédiction des FN et FP) pour comparer les modèles, afin de choisir le meilleur modèle et ses meilleurs hyperparamètres.\n",
    "\n",
    "**Precision**  \n",
    "(tp / (tp + fp))\n",
    "It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value.  \n",
    "\n",
    "**Recall** is the same as sensitivity.\n",
    "(tp / (tp + fn)    \n",
    "Recall describes how good the model is at predicting the positive class when the actual outcome is positive.\n",
    "\n",
    "\n",
    "We want to optimize recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d982f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fbeta_score_calculation(y_true, y_pred):\n",
    "    fbeta_macro = fbeta_score(y_true, y_pred, average='macro', beta=2)\n",
    "    fbeta_micro = fbeta_score(y_true, y_pred, average='micro', beta=2)\n",
    "    fbeta_weighted = fbeta_score(y_true, y_pred, average='weighted', beta=2)\n",
    "    return round(fbeta_macro,2), round(fbeta_micro,2), round(fbeta_weighted,2)\n",
    "\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "def reminder_TP(TN=\"\", FP=\"\", FN=\"\", TP=\"\"):\n",
    "    #tn, fp, fn, tp\n",
    "    # Create a PrettyTable instance\n",
    "    table = PrettyTable()\n",
    "\n",
    "    # Define columns and headers\n",
    "    table.field_names = ['Confusion Matrix', 'Positive prediction', 'Negative prediction']\n",
    "\n",
    "    # Add rows\n",
    "    table.add_row(['Positive class', 'True positive (TP)', 'False negative (FN)'])\n",
    "    table.add_row(['Negative class', 'False positive (FP)', 'True negative (TN)'])\n",
    "\n",
    "    # Print the table\n",
    "    print(table)\n",
    "    \n",
    "    if TN :\n",
    "        # Create a PrettyTable instance\n",
    "        table2 = PrettyTable()\n",
    "\n",
    "        # Define columns and headers\n",
    "        table2.field_names = ['Confusion Matrix', 'Positive prediction', 'Negative prediction']\n",
    "\n",
    "        # Add rows\n",
    "        table2.add_row(['Positive class', TP, FN])\n",
    "        table2.add_row(['Negative class', FP, TN])\n",
    "\n",
    "        # Print the table\n",
    "        print(table2)\n",
    "    \n",
    "\n",
    "def generate_recall_precision_curve(model, X_test, Y_test):\n",
    "    # predict probabilities\n",
    "    lr_probs = model.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "    yhat = model.predict(X_test)\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(Y_test, lr_probs)\n",
    "    lr_f1, lr_auc = f1_score(Y_test, yhat), auc(lr_recall, lr_precision)\n",
    "    # summarize scores\n",
    "    print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(Y_test[Y_test==1]) / len(Y_test)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "def generate_auc_roc_curve(clf, X_test):\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,  y_pred_proba)\n",
    "    auc = roc_auc_score(Y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "def generate_model_report(model, model_name, X_test, Y_test):\n",
    "    \n",
    "    Y_Test_Pred_best_param = model.predict(X_test)\n",
    "    \n",
    "    generate_auc_roc_curve(model, X_test)\n",
    "    generate_recall_precision_curve(model, X_test, Y_test)\n",
    "    \n",
    "    #cm_tab = pd.crosstab(pd.Series(Y_Test_Pred_best_param, name = 'Predicted'), \n",
    "     #       pd.Series(Y_test, name = 'Actual'))\n",
    "    #print(cm_tab)\n",
    "    \n",
    "    cm = confusion_matrix(np.array(Y_test), Y_Test_Pred_best_param  )\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    reminder_TP(tn, fp, fn, tp)\n",
    "    \n",
    "    roc_auc = roc_auc_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "\n",
    "    bu_Sc = fp + (10*fn) #business score FN cost 10 times more than fp\n",
    "    ac_Sc = accuracy_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    pr_Sc = precision_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    re_Sc = recall_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    F1_Sc = f1_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    fbeta_macro, fbeta_micro, fbeta_weighted = fbeta_score_calculation(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    print('ROC AUC: ', roc_auc)\n",
    "    print(\"Accuracy = \" , accuracy_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"Precision = \" ,precision_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"Recall = \" ,recall_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"F1 Score = \" ,f1_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"Fbeta Score = \" ,fbeta_score_calculation(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    \n",
    "    metrics = pd.DataFrame({'model': model_name,'tn': [tn], 'fp': [fp], 'fn': [fn], 'tp': [tp],'FP+10*FN': bu_Sc,\n",
    "                            'accuracy': [ac_Sc], \n",
    "                            'ROC_AUC': [roc_auc],\n",
    "                            'precision': [pr_Sc],\n",
    "                            'recall': [re_Sc],\n",
    "                            'F1_Score': [F1_Sc],\n",
    "                            'Fbeta_macro':[fbeta_macro], \n",
    "                            'Fbeta_micro':[fbeta_micro],\n",
    "                            'Fbeta_weighted':[fbeta_weighted]\n",
    "                            })\n",
    "    print(metrics)\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e340e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC_model(X_train, Y_train):\n",
    "    # Define the RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    # Define the hyperparameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Get the best hyperparameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train the RandomForestClassifier with the best hyperparameters on the entire training set\n",
    "    best_rf_classifier = RandomForestClassifier(**best_params)\n",
    "    best_rf_classifier.fit(X_train, Y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f'Best Hyperparameters: {best_params}')\n",
    "    print(f'Accuracy on Test Set: {accuracy}')\n",
    "    \n",
    "    return best_rf_classifier, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e3f40",
   "metadata": {},
   "source": [
    "**RFC on Smote data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b273236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markobriesemann/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m RFC_model_smote, best_params_smote \u001b[38;5;241m=\u001b[39m \u001b[43mRFC_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_smote\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mRFC_model\u001b[0;34m(X_train, Y_train)\u001b[0m\n\u001b[1;32m     14\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_classifier, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform the grid search on the training data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters from the grid search\u001b[39;00m\n\u001b[1;32m     20\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py:434\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RFC_model_smote, best_params_smote = RFC_model(x_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b391492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_smote = generate_model_report(RFC_model_smote, \"RFC\", X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1bda8",
   "metadata": {},
   "source": [
    "**Test RFC on original data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef859590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RFC_model, best_params = RFC_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aa82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e65b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accessing feature importance\n",
    "feature_importance = RFC_model.feature_importances_\n",
    "\n",
    "# Printing feature importance for each feature\n",
    "for i, feature_name in enumerate(feature_names.iloc[:,0].values.tolist()):\n",
    "    print(f'Feature: {feature_name}, Importance: {feature_importance[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names.iloc[:,0].values.tolist(),\n",
    "    'Importance': RFC_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame by importance scores\n",
    "feature_importance_df_sorted = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Displaying the sorted DataFrame\n",
    "#print(feature_importance_df_sorted.head(20))\n",
    "feature_importance_df_sorted['Feature'][:20].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(RFC_model, 'loan_default_model.joblib')\n",
    "\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e9219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_rf_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c2a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_model_report(best_rf_classifier, \"RFC\", X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf49b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_business_score(predictions_proba, Y_true):\n",
    "    \n",
    "    print(\"prediction proba\", len(predictions_proba))\n",
    "    print(\"Y_true\", len(Y_true))\n",
    "\n",
    "    # Threshold values from 0 to 0.5\n",
    "    threshold_values = [i / 10 for i in range(6)]\n",
    "    best_B_score = 100000\n",
    "    \n",
    "    # Create an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['threshold', 'tn', 'fp', 'fn', 'tp', 'FP+10*FN', 'accuracy', 'ROC_AUC', 'precision', 'recall', 'F1_Score', 'Fbeta_macro', 'Fbeta_micro', 'Fbeta_weighted','best'])\n",
    "\n",
    "    # Loop through threshold values\n",
    "    for threshold in threshold_values:\n",
    "        best = 0\n",
    "        # Convert probabilities to binary predictions based on threshold\n",
    "        predicted_labels = [1 if x[1] >= threshold else 0 for x in predictions_proba]\n",
    "\n",
    "        # Calculate confusion matrix and other metrics\n",
    "        cm = confusion_matrix(Y_true, predicted_labels)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        FP_10_FN = fp + 10 * fn\n",
    "        precision = precision_score(Y_true, predicted_labels)\n",
    "        recall = recall_score(Y_true, predicted_labels)\n",
    "        accuracy = accuracy_score(Y_true, predicted_labels)\n",
    "        roc_auc = roc_auc_score(Y_true, predicted_labels)\n",
    "        f1 = f1_score(Y_true, predicted_labels)\n",
    "        fbeta_macro = fbeta_score(Y_true, predicted_labels, beta=2, average='macro')\n",
    "        fbeta_micro = fbeta_score(Y_true, predicted_labels, beta=2, average='micro')\n",
    "        fbeta_weighted = fbeta_score(Y_true, predicted_labels, beta=2, average='weighted')\n",
    "\n",
    "        # best param\n",
    "        if (best_B_score > FP_10_FN ):\n",
    "            print(results_df[results_df['best']==1]['best'])\n",
    "            results_df.loc[(results_df['best']==1),'best']  = 0\n",
    "            best_B_score = FP_10_FN\n",
    "            best = 1\n",
    "        \n",
    "        \n",
    "        # Create a DataFrame for the current threshold iteration\n",
    "        data = {'threshold': [threshold],\n",
    "                'tn': [tn],\n",
    "                'fp': [fp],\n",
    "                'fn': [fn],\n",
    "                'tp': [tp],\n",
    "                'FP+10*FN': [FP_10_FN],\n",
    "                'accuracy': [accuracy],\n",
    "                'ROC_AUC': [roc_auc],\n",
    "                'precision': [precision],\n",
    "                'recall': [recall],\n",
    "                'F1_Score': [f1],\n",
    "                'Fbeta_macro': [fbeta_macro],\n",
    "                'Fbeta_micro': [fbeta_micro],\n",
    "                'Fbeta_weighted': [fbeta_weighted],\n",
    "                'best': best\n",
    "               }\n",
    "\n",
    "        threshold_df = pd.DataFrame(data)\n",
    "\n",
    "        # Concatenate the current threshold results to the overall results DataFrame\n",
    "        results_df = pd.concat([results_df, threshold_df], ignore_index=True)\n",
    "        \n",
    "    print(\"best b score\", best_B_score, results_df[results_df['best']==1]['threshold'])\n",
    "    print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = find_optimal_business_score(y_pred_proba, Y_test)\n",
    "metrics_domain = { \"train\": metrics[\"train\"][5], \n",
    "                  \"valid\": metrics[\"valid\"][5],\n",
    "                  \"TN\":metrics_FN_FP[\"TN\"][0],\n",
    "                  \"FP\":metrics_FN_FP[\"FP\"][0],\n",
    "                  \"FN\":metrics_FN_FP[\"FN\"][0],\n",
    "                  \"TP\":metrics_FN_FP[\"TP\"][0],\n",
    "                  \"Accuracy\": accuracy,\n",
    "                 \"F1\":f1}\n",
    "print(metrics_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd96d7e",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217082f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #mlflow\n",
    "  \n",
    "print(\"Save model with MLflow\")\n",
    "signature = infer_signature(domain_features, train_labels)\n",
    "mlflow.sklearn.save_model(model, 'mlflow_model_RF', signature=signature)\n",
    "\n",
    "\n",
    "experiment_name = \"RF_Models_01\"  \n",
    "run_name = \"RF_01_testMLflow\"\n",
    "run_MLflow(experiment_name, run_name, metrics_domain, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:8080/\")\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"This is the credit score project. \"\n",
    "    \"This experiment contains the basic LightGBM model.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"credit-score-classification\",\n",
    "    \"store_dept\": \"Prêt à dépenser\",\n",
    "    \"team\": \"cred-ml\",\n",
    "    \"project_quarter\": \"Q1-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "credit_classification_LightGBM_experiment = client.create_experiment(\n",
    "    name=\"LightGBM_Models_01\", tags=experiment_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def run_MLflow(experiment_name, run_name, metrics, params, model_obj, X_val, signature):\n",
    "\n",
    "    # Sets the current active experiment to the \"LightGBM_Models\" experiment and\n",
    "    # returns the Experiment metadata\n",
    "    LightGBM_experiment = mlflow.set_experiment(\"LightGBM_Models_01\")\n",
    "\n",
    "    # Define an artifact path that the model will be saved to.\n",
    "    artifact_path = \"LGBM_test\"\n",
    "\n",
    "    # Assemble the metrics we're going to write into a collection\n",
    "    #metrics = { \"train\": metrics_domain[\"train\"][5], \"valid\": metrics_domain[\"valid\"][5]}\n",
    "    print(metrics)\n",
    "\n",
    "    # Define a run name for this iteration of training.\n",
    "    # If this is not set, a unique name will be auto-generated for your run.\n",
    "    run_name = run_name\n",
    "\n",
    "    # Initiate the MLflow run context\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Log the parameters used for the model fit\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log the error metrics that were calculated during validation\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Log an instance of the trained model for later use\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model_obj, input_example=X_val, artifact_path=artifact_path\n",
    "        )\n",
    "        # Auto log all MLflow entities\n",
    "        #mlflow.lightgbm.autolog()\n",
    "        #model_info = mlflow.lightgbm.log_model(model, artifact_path, signature=signature)\n",
    "\n",
    "        \n",
    "\n",
    "best_params = PARAMS\n",
    "experiment_name = \"LightGBM_Models_02\"  \n",
    "run_name = \"LightGBM_05_testMLflow_avecSignature_F1_accuracy\"\n",
    "#run_MLflow(experiment_name, run_name, metrics_domain, best_params)\n",
    "run_MLflow(experiment_name, run_name, metrics_domain, best_params, model_LGBM, X_train, signature)\n",
    "#run_MLflow(experiment_name, run_name, metrics_domain, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233f52e",
   "metadata": {},
   "source": [
    "## Test MLflow on Fastapi model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b56318",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:8000/predict -H 'Content-Type: application/json' -d '{\"inputs\": [[0, 0, 1, 1, 63000.0, 310500.0, 15232.5, 310500.0, 0.026392, 16263, -214.0, -8930.0, -573, 0.0, 1, 1, 0, 1, 1, 0, 2.0, 2, 2, 11, 0, 0, 0, 0, 1, 1, 0.0, 0.0765011930557638, 0.0005272652387098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false]]}'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2780b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":[{\"type\":\"float_type\",\"loc\":[\"body\",\"data_point\",0],\"msg\":\"Input should be a valid number\",\"input\":[0,0,1,1,63000.0,310500.0,15232.5,310500.0,0.026392,16263,-214.0,-8930.0,-573,0.0,1,1,0,1,1,0,2.0,2,2,11,0,0,0,0,1,1,0.0,0.0765011930557638,0.0005272652387098,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,true,false,false,false,false,false,false,false,true,false,false,false,false,false,false,true,false,false,false,false,true,false,false,false,true,false,false,true,false,false,false,false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false],\"url\":\"https://errors.pydantic.dev/2.6/v/float_type\"}]}"
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:8000/predict -H 'Content-Type: application/json' -d '{\"data_point\": [[0, 0, 1, 1, 63000.0, 310500.0, 15232.5, 310500.0, 0.026392, 16263, -214.0, -8930.0, -573, 0.0, 1, 1, 0, 1, 1, 0, 2.0, 2, 2, 11, 0, 0, 0, 0, 1, 1, 0.0, 0.0765011930557638, 0.0005272652387098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false]]}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173c5e8",
   "metadata": {},
   "source": [
    "**Conversion of type of the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16023232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246008 entries, 0 to 246007\n",
      "Columns: 239 entries, 0 to 238\n",
      "dtypes: float64(239)\n",
      "memory usage: 448.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Select columns with data type 'int64'\n",
    "int_columns = X_train.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Convert selected columns to int\n",
    "X_train[int_columns] = X_train[int_columns].astype('float')\n",
    "# Select columns with data type 'int64'\n",
    "int_columns = X_train.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Convert selected columns to int\n",
    "X_train[int_columns] = X_train[int_columns].astype('float')\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52314e2d",
   "metadata": {},
   "source": [
    "**Selection of a data point for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4718bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train.copy()\n",
    "test[\"ID\"] = ids_test\n",
    "test.set_index(\"ID\", inplace=True)\n",
    "ids_test.iloc[5]\n",
    "#test.loc[100008].values.tolist()\n",
    "data_for_request = test.loc[100030].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a4a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_for_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d5b7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI :  http://127.0.0.1:8000/predict\n",
      "Predictions: {\"prediction\":0.8939533102108367,\"probability\":0.8}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# initialised with: mlflow models serve -m model_LGBM02/ --port 8092\n",
    "#http://127.0.0.1:8092\n",
    "\n",
    "host = '127.0.0.1'\n",
    "port = '8000'\n",
    "\n",
    "# endpoint\n",
    "url = f'http://{host}:{port}/predict'\n",
    "print(\"URI : \", url)\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# Send the POST request with the data\n",
    "response = requests.post(url, json={\"data_point\": data_for_request})\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9329b5",
   "metadata": {},
   "source": [
    "**TEST with empty data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3773827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {\"detail\":\"An error occurred during prediction: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.\"}\n"
     ]
    }
   ],
   "source": [
    "# Send the POST request with the data\n",
    "response = requests.post(url, json={\"data_point\":[]})\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9c2b4",
   "metadata": {},
   "source": [
    "**TEST on hosting environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://fastapi-cd-webapp.azurewebsites.net/predict'\n",
    "# Send the POST request with the data\n",
    "response = requests.post(url, json={\"data_point\": data_for_request})\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be404e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
