{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8b0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee5e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (246008, 239)\n",
      "Y_train (246008, 1)\n",
      "Y_test (61503, 1)\n",
      "ids_test (246008, 1)\n",
      "feature names (239, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('../2_INPUT_DATA/3_SPLIT/X_train.csv')\n",
    "Y_train = pd.read_csv('../2_INPUT_DATA/3_SPLIT/Y_train.csv')\n",
    "X_test = pd.read_csv('../2_INPUT_DATA/3_SPLIT/X_test.csv')\n",
    "Y_test = pd.read_csv('../2_INPUT_DATA/3_SPLIT/Y_test.csv')\n",
    "ids_test = pd.read_csv('../2_INPUT_DATA/3_SPLIT/ids_test.csv')\n",
    "feature_names = pd.read_csv('../2_INPUT_DATA/2_FEATURE_PROCESSED/feature_names.csv')\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"Y_train\", Y_train.shape)\n",
    "print(\"Y_test\", Y_test.shape)\n",
    "print(\"ids_test\",ids_test.shape)\n",
    "print(\"feature names\", feature_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bede7c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 99000.0,\n",
       " 490495.5,\n",
       " 27517.5,\n",
       " 454500.0,\n",
       " 0.035792,\n",
       " 16941,\n",
       " -1588.0,\n",
       " -4970.0,\n",
       " -477,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2.0,\n",
       " 2,\n",
       " 2,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.3542247319929012,\n",
       " 0.6212263380626669,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -2536.0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = X_train.copy()\n",
    "test[\"ID\"] = ids_test\n",
    "test.set_index(\"ID\", inplace=True)\n",
    "ids_test.iloc[5]\n",
    "test.loc[100008].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee29821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae50308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 226067, 1: 19941})\n",
      "Counter({0: 45212, 1: 22606})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def SMOTE_transformation(X, y):\n",
    "    # summarize class distribution\n",
    "    counter = Counter(y)\n",
    "    print(counter)\n",
    "    # define pipeline\n",
    "    over = SMOTE(sampling_strategy=0.1)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    # transform the dataset\n",
    "    X, y = pipeline.fit_resample(X, y)\n",
    "    # summarize the new class distribution\n",
    "    counter = Counter(y)\n",
    "    print(counter)\n",
    "    # scatter plot of examples by class label\n",
    "    #for label, _ in counter.items():\n",
    "    #    print(label)\n",
    "    #    row_ix = where(y == label)[0]\n",
    "    #    print(row_ix)\n",
    "    #    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "        \n",
    "    #pyplot.legend()\n",
    "    #pyplot.show()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "x_train_smote, y_train_smote = SMOTE_transformation(X_train, Y_train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1cdbe1",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "**Fbeta Score**\n",
    "\n",
    "![alt text for screen readers](IMAGES/fbetascore.png \"Fbeta Score\")\n",
    "\n",
    "A smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score.\n",
    "\n",
    "#### FN et FP  \n",
    "Vous pourrez supposer, par exemple, que le coût d’un FN est dix fois supérieur au coût d’un FP.  \n",
    "Vous créerez un score “métier” (minimisation du coût d’erreur de prédiction des FN et FP) pour comparer les modèles, afin de choisir le meilleur modèle et ses meilleurs hyperparamètres.\n",
    "\n",
    "**Precision**  \n",
    "(tp / (tp + fp))\n",
    "It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value.  \n",
    "\n",
    "**Recall** is the same as sensitivity.\n",
    "(tp / (tp + fn)    \n",
    "Recall describes how good the model is at predicting the positive class when the actual outcome is positive.\n",
    "\n",
    "\n",
    "We want to optimize recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d982f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fbeta_score_calculation(y_true, y_pred):\n",
    "    fbeta_macro = fbeta_score(y_true, y_pred, average='macro', beta=2)\n",
    "    fbeta_micro = fbeta_score(y_true, y_pred, average='micro', beta=2)\n",
    "    fbeta_weighted = fbeta_score(y_true, y_pred, average='weighted', beta=2)\n",
    "    return round(fbeta_macro,2), round(fbeta_micro,2), round(fbeta_weighted,2)\n",
    "\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "def reminder_TP(TN=\"\", FP=\"\", FN=\"\", TP=\"\"):\n",
    "    #tn, fp, fn, tp\n",
    "    # Create a PrettyTable instance\n",
    "    table = PrettyTable()\n",
    "\n",
    "    # Define columns and headers\n",
    "    table.field_names = ['Confusion Matrix', 'Positive prediction', 'Negative prediction']\n",
    "\n",
    "    # Add rows\n",
    "    table.add_row(['Positive class', 'True positive (TP)', 'False negative (FN)'])\n",
    "    table.add_row(['Negative class', 'False positive (FP)', 'True negative (TN)'])\n",
    "\n",
    "    # Print the table\n",
    "    print(table)\n",
    "    \n",
    "    if TN :\n",
    "        # Create a PrettyTable instance\n",
    "        table2 = PrettyTable()\n",
    "\n",
    "        # Define columns and headers\n",
    "        table2.field_names = ['Confusion Matrix', 'Positive prediction', 'Negative prediction']\n",
    "\n",
    "        # Add rows\n",
    "        table2.add_row(['Positive class', TP, FN])\n",
    "        table2.add_row(['Negative class', FP, TN])\n",
    "\n",
    "        # Print the table\n",
    "        print(table2)\n",
    "    \n",
    "\n",
    "def generate_recall_precision_curve(model, X_test, Y_test):\n",
    "    # predict probabilities\n",
    "    lr_probs = model.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "    yhat = model.predict(X_test)\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(Y_test, lr_probs)\n",
    "    lr_f1, lr_auc = f1_score(Y_test, yhat), auc(lr_recall, lr_precision)\n",
    "    # summarize scores\n",
    "    print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(Y_test[Y_test==1]) / len(Y_test)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "def generate_auc_roc_curve(clf, X_test):\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,  y_pred_proba)\n",
    "    auc = roc_auc_score(Y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "def generate_model_report(model, model_name, X_test, Y_test):\n",
    "    \n",
    "    Y_Test_Pred_best_param = model.predict(X_test)\n",
    "    \n",
    "    generate_auc_roc_curve(model, X_test)\n",
    "    generate_recall_precision_curve(model, X_test, Y_test)\n",
    "    \n",
    "    #cm_tab = pd.crosstab(pd.Series(Y_Test_Pred_best_param, name = 'Predicted'), \n",
    "     #       pd.Series(Y_test, name = 'Actual'))\n",
    "    #print(cm_tab)\n",
    "    \n",
    "    cm = confusion_matrix(np.array(Y_test), Y_Test_Pred_best_param  )\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    reminder_TP(tn, fp, fn, tp)\n",
    "    \n",
    "    roc_auc = roc_auc_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "\n",
    "    bu_Sc = fp + (10*fn) #business score FN cost 10 times more than fp\n",
    "    ac_Sc = accuracy_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    pr_Sc = precision_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    re_Sc = recall_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    F1_Sc = f1_score(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    fbeta_macro, fbeta_micro, fbeta_weighted = fbeta_score_calculation(np.array(Y_test), Y_Test_Pred_best_param)\n",
    "    print('ROC AUC: ', roc_auc)\n",
    "    print(\"Accuracy = \" , accuracy_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"Precision = \" ,precision_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"Recall = \" ,recall_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"F1 Score = \" ,f1_score(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    print(\"Fbeta Score = \" ,fbeta_score_calculation(np.array(Y_test), Y_Test_Pred_best_param))\n",
    "    \n",
    "    metrics = pd.DataFrame({'model': model_name,'tn': [tn], 'fp': [fp], 'fn': [fn], 'tp': [tp],'FP+10*FN': bu_Sc,\n",
    "                            'accuracy': [ac_Sc], \n",
    "                            'ROC_AUC': [roc_auc],\n",
    "                            'precision': [pr_Sc],\n",
    "                            'recall': [re_Sc],\n",
    "                            'F1_Score': [F1_Sc],\n",
    "                            'Fbeta_macro':[fbeta_macro], \n",
    "                            'Fbeta_micro':[fbeta_micro],\n",
    "                            'Fbeta_weighted':[fbeta_weighted]\n",
    "                            })\n",
    "    print(metrics)\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e340e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC_model(X_train, Y_train):\n",
    "    # Define the RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    # Define the hyperparameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Get the best hyperparameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train the RandomForestClassifier with the best hyperparameters on the entire training set\n",
    "    best_rf_classifier = RandomForestClassifier(**best_params)\n",
    "    best_rf_classifier.fit(X_train, Y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f'Best Hyperparameters: {best_params}')\n",
    "    print(f'Accuracy on Test Set: {accuracy}')\n",
    "    \n",
    "    return best_rf_classifier, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e3f40",
   "metadata": {},
   "source": [
    "**RFC on Smote data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b273236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markobriesemann/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "RFC_model_smote, best_params_smote = RFC_model(x_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b391492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_smote = generate_model_report(RFC_model_smote, \"RFC\", X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1bda8",
   "metadata": {},
   "source": [
    "**Test RFC on original data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef859590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RFC_model, best_params = RFC_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aa82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e65b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accessing feature importance\n",
    "feature_importance = RFC_model.feature_importances_\n",
    "\n",
    "# Printing feature importance for each feature\n",
    "for i, feature_name in enumerate(feature_names.iloc[:,0].values.tolist()):\n",
    "    print(f'Feature: {feature_name}, Importance: {feature_importance[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names.iloc[:,0].values.tolist(),\n",
    "    'Importance': RFC_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame by importance scores\n",
    "feature_importance_df_sorted = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Displaying the sorted DataFrame\n",
    "#print(feature_importance_df_sorted.head(20))\n",
    "feature_importance_df_sorted['Feature'][:20].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(RFC_model, 'loan_default_model.joblib')\n",
    "\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e9219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_rf_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c2a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_model_report(best_rf_classifier, \"RFC\", X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf49b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_business_score(predictions_proba, Y_true):\n",
    "    \n",
    "    print(\"prediction proba\", len(predictions_proba))\n",
    "    print(\"Y_true\", len(Y_true))\n",
    "\n",
    "    # Threshold values from 0 to 0.5\n",
    "    threshold_values = [i / 10 for i in range(6)]\n",
    "    best_B_score = 100000\n",
    "    \n",
    "    # Create an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['threshold', 'tn', 'fp', 'fn', 'tp', 'FP+10*FN', 'accuracy', 'ROC_AUC', 'precision', 'recall', 'F1_Score', 'Fbeta_macro', 'Fbeta_micro', 'Fbeta_weighted','best'])\n",
    "\n",
    "    # Loop through threshold values\n",
    "    for threshold in threshold_values:\n",
    "        best = 0\n",
    "        # Convert probabilities to binary predictions based on threshold\n",
    "        predicted_labels = [1 if x[1] >= threshold else 0 for x in predictions_proba]\n",
    "\n",
    "        # Calculate confusion matrix and other metrics\n",
    "        cm = confusion_matrix(Y_true, predicted_labels)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        FP_10_FN = fp + 10 * fn\n",
    "        precision = precision_score(Y_true, predicted_labels)\n",
    "        recall = recall_score(Y_true, predicted_labels)\n",
    "        accuracy = accuracy_score(Y_true, predicted_labels)\n",
    "        roc_auc = roc_auc_score(Y_true, predicted_labels)\n",
    "        f1 = f1_score(Y_true, predicted_labels)\n",
    "        fbeta_macro = fbeta_score(Y_true, predicted_labels, beta=2, average='macro')\n",
    "        fbeta_micro = fbeta_score(Y_true, predicted_labels, beta=2, average='micro')\n",
    "        fbeta_weighted = fbeta_score(Y_true, predicted_labels, beta=2, average='weighted')\n",
    "\n",
    "        # best param\n",
    "        if (best_B_score > FP_10_FN ):\n",
    "            print(results_df[results_df['best']==1]['best'])\n",
    "            results_df.loc[(results_df['best']==1),'best']  = 0\n",
    "            best_B_score = FP_10_FN\n",
    "            best = 1\n",
    "        \n",
    "        \n",
    "        # Create a DataFrame for the current threshold iteration\n",
    "        data = {'threshold': [threshold],\n",
    "                'tn': [tn],\n",
    "                'fp': [fp],\n",
    "                'fn': [fn],\n",
    "                'tp': [tp],\n",
    "                'FP+10*FN': [FP_10_FN],\n",
    "                'accuracy': [accuracy],\n",
    "                'ROC_AUC': [roc_auc],\n",
    "                'precision': [precision],\n",
    "                'recall': [recall],\n",
    "                'F1_Score': [f1],\n",
    "                'Fbeta_macro': [fbeta_macro],\n",
    "                'Fbeta_micro': [fbeta_micro],\n",
    "                'Fbeta_weighted': [fbeta_weighted],\n",
    "                'best': best\n",
    "               }\n",
    "\n",
    "        threshold_df = pd.DataFrame(data)\n",
    "\n",
    "        # Concatenate the current threshold results to the overall results DataFrame\n",
    "        results_df = pd.concat([results_df, threshold_df], ignore_index=True)\n",
    "        \n",
    "    print(\"best b score\", best_B_score, results_df[results_df['best']==1]['threshold'])\n",
    "    print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = find_optimal_business_score(y_pred_proba, Y_test)\n",
    "metrics_domain = { \"train\": metrics[\"train\"][5], \n",
    "                  \"valid\": metrics[\"valid\"][5],\n",
    "                  \"TN\":metrics_FN_FP[\"TN\"][0],\n",
    "                  \"FP\":metrics_FN_FP[\"FP\"][0],\n",
    "                  \"FN\":metrics_FN_FP[\"FN\"][0],\n",
    "                  \"TP\":metrics_FN_FP[\"TP\"][0],\n",
    "                  \"Accuracy\": accuracy,\n",
    "                 \"F1\":f1}\n",
    "print(metrics_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd96d7e",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217082f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #mlflow\n",
    "  \n",
    "print(\"Save model with MLflow\")\n",
    "signature = infer_signature(domain_features, train_labels)\n",
    "mlflow.sklearn.save_model(model, 'mlflow_model_RF', signature=signature)\n",
    "\n",
    "\n",
    "experiment_name = \"RF_Models_01\"  \n",
    "run_name = \"RF_01_testMLflow\"\n",
    "run_MLflow(experiment_name, run_name, metrics_domain, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:8080/\")\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"This is the credit score project. \"\n",
    "    \"This experiment contains the basic LightGBM model.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"credit-score-classification\",\n",
    "    \"store_dept\": \"Prêt à dépenser\",\n",
    "    \"team\": \"cred-ml\",\n",
    "    \"project_quarter\": \"Q1-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "credit_classification_LightGBM_experiment = client.create_experiment(\n",
    "    name=\"LightGBM_Models_01\", tags=experiment_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def run_MLflow(experiment_name, run_name, metrics, params, model_obj, X_val, signature):\n",
    "\n",
    "    # Sets the current active experiment to the \"LightGBM_Models\" experiment and\n",
    "    # returns the Experiment metadata\n",
    "    LightGBM_experiment = mlflow.set_experiment(\"LightGBM_Models_01\")\n",
    "\n",
    "    # Define an artifact path that the model will be saved to.\n",
    "    artifact_path = \"LGBM_test\"\n",
    "\n",
    "    # Assemble the metrics we're going to write into a collection\n",
    "    #metrics = { \"train\": metrics_domain[\"train\"][5], \"valid\": metrics_domain[\"valid\"][5]}\n",
    "    print(metrics)\n",
    "\n",
    "    # Define a run name for this iteration of training.\n",
    "    # If this is not set, a unique name will be auto-generated for your run.\n",
    "    run_name = run_name\n",
    "\n",
    "    # Initiate the MLflow run context\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Log the parameters used for the model fit\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log the error metrics that were calculated during validation\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Log an instance of the trained model for later use\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model_obj, input_example=X_val, artifact_path=artifact_path\n",
    "        )\n",
    "        # Auto log all MLflow entities\n",
    "        #mlflow.lightgbm.autolog()\n",
    "        #model_info = mlflow.lightgbm.log_model(model, artifact_path, signature=signature)\n",
    "\n",
    "        \n",
    "\n",
    "best_params = PARAMS\n",
    "experiment_name = \"LightGBM_Models_02\"  \n",
    "run_name = \"LightGBM_05_testMLflow_avecSignature_F1_accuracy\"\n",
    "#run_MLflow(experiment_name, run_name, metrics_domain, best_params)\n",
    "run_MLflow(experiment_name, run_name, metrics_domain, best_params, model_LGBM, X_train, signature)\n",
    "#run_MLflow(experiment_name, run_name, metrics_domain, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233f52e",
   "metadata": {},
   "source": [
    "## Test MLflow on Fastapi model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b56318",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:8000/predict -H 'Content-Type: application/json' -d '{\"inputs\": [[0, 0, 1, 1, 63000.0, 310500.0, 15232.5, 310500.0, 0.026392, 16263, -214.0, -8930.0, -573, 0.0, 1, 1, 0, 1, 1, 0, 2.0, 2, 2, 11, 0, 0, 0, 0, 1, 1, 0.0, 0.0765011930557638, 0.0005272652387098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false]]}'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2780b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:8000/predict -H 'Content-Type: application/json' -d '{\"data_point\": [[0, 0, 1, 1, 63000.0, 310500.0, 15232.5, 310500.0, 0.026392, 16263, -214.0, -8930.0, -573, 0.0, 1, 1, 0, 1, 1, 0, 2.0, 2, 2, 11, 0, 0, 0, 0, 1, 1, 0.0, 0.0765011930557638, 0.0005272652387098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false]]}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173c5e8",
   "metadata": {},
   "source": [
    "**Conversion of type of the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16023232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with data type 'int64'\n",
    "int_columns = X_train.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Convert selected columns to int\n",
    "X_train[int_columns] = X_train[int_columns].astype('float')\n",
    "# Select columns with data type 'int64'\n",
    "int_columns = X_train.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Convert selected columns to int\n",
    "X_train[int_columns] = X_train[int_columns].astype('float')\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52314e2d",
   "metadata": {},
   "source": [
    "**Selection of a data point for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train.copy()\n",
    "test[\"ID\"] = ids_test\n",
    "test.set_index(\"ID\", inplace=True)\n",
    "ids_test.iloc[5]\n",
    "#test.loc[100008].values.tolist()\n",
    "data_for_request = test.loc[100030].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_for_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# initialised with: mlflow models serve -m model_LGBM02/ --port 8092\n",
    "#http://127.0.0.1:8092\n",
    "\n",
    "host = '127.0.0.1'\n",
    "port = '8000'\n",
    "\n",
    "# endpoint\n",
    "url = f'http://{host}:{port}/predict'\n",
    "print(\"URI : \", url)\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# Send the POST request with the data\n",
    "response = requests.post(url, json={\"data_point\": data_for_request})\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9329b5",
   "metadata": {},
   "source": [
    "**TEST with empty data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the POST request with the data\n",
    "response = requests.post(url, json={\"data_point\":[]})\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9c2b4",
   "metadata": {},
   "source": [
    "**TEST on hosting environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://fastapi-cd-webapp.azurewebsites.net/predict'\n",
    "# Send the POST request with the data\n",
    "response = requests.post(url, json={\"data_point\": data_for_request})\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be404e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
